# Architecture Deep Dive: Aura AI-Driven Under-utilized Resource Autoscaler

## 1. Big Picture

**Project Type:** Infrastructure-as-Code (IaC) automation system for ephemeral Kubernetes clusters

**Problem Solved:**
This project solves the challenge of running cost-efficient, on-demand AI/ML workloads (particularly LLM jobs) on AWS EKS. Instead of maintaining expensive, always-on GPU clusters, it provides a fully automated system that:
- Spins up Kubernetes clusters on-demand
- Automatically provisions compute nodes when workloads need them (via Karpenter)
- Runs jobs and collects results
- Tears down everything when done (zero idle cost)

**Target Use Case:** Ephemeral AI workloads that need GPU resources but don't require persistent infrastructure.

---

## 2. Core Architecture

### Architecture Pattern: **Modular Infrastructure-as-Code with CI/CD Orchestration**

The system follows a **layered, modular architecture** with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CI/CD Layer (GitHub Actions)              â”‚
â”‚  Orchestrates: Deploy â†’ Run â†’ Collect â†’ Destroy              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Layer (Terraform)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   VPC    â”‚â†’ â”‚   EKS    â”‚â†’ â”‚   IAM    â”‚â†’ â”‚  OIDC   â”‚    â”‚
â”‚  â”‚  Module  â”‚  â”‚  Module  â”‚  â”‚  Module  â”‚  â”‚ Providerâ”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Kubernetes Layer (EKS + Karpenter)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   EKS        â”‚         â”‚  Karpenter   â”‚                  â”‚
â”‚  â”‚  Cluster     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Controller â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚       â”‚                          â”‚                          â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                  â–¼                                            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚         â”‚  EC2 Nodes       â”‚                                 â”‚
â”‚         â”‚  (Auto-provision)â”‚                                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure

```
Aura_AI-driven-Under-utilized-Resource-Autoscaler/
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â””â”€â”€ dev/                    # Root module (orchestrates all modules)
â”‚   â”‚       â”œâ”€â”€ main.tf             # Module composition & wiring
â”‚   â”‚       â”œâ”€â”€ outputs.tf          # Exports values for CI/CD
â”‚   â”‚       â”œâ”€â”€ get-oidc-thumbprint.py  # Helper script
â”‚   â”‚       â””â”€â”€ karpenter-controller-policy.json  # IAM policy
â”‚   â””â”€â”€ modules/                    # Reusable Terraform modules
â”‚       â”œâ”€â”€ VPC/                    # Networking infrastructure
â”‚       â”œâ”€â”€ EKS/                    # Kubernetes cluster
â”‚       â”œâ”€â”€ IAM/                    # Karpenter IAM roles
â”‚       â””â”€â”€ IAM_EKS/                # EKS cluster/node IAM roles
â”œâ”€â”€ Karpenter/
â”‚   â”œâ”€â”€ main.yml                    # Karpenter Provisioner CRD
â”‚   â””â”€â”€ app-job.yml                 # Example Kubernetes Job
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ cd-cd.yml               # CI/CD pipeline
â””â”€â”€ env.example                     # Environment variables template
```

---

## 3. Key Components

### 3.1 VPC Module (`terraform/modules/VPC/`)

**Purpose:** Creates the network foundation for the EKS cluster

**What it creates:**
- VPC with DNS support
- Public subnets (2) with Internet Gateway
- Private subnets (2) with NAT Gateway
- Route tables and associations
- **Critical:** Tags subnets with `karpenter.sh/discovery` for Karpenter discovery

**Key Design Decision:** Both public and private subnets are tagged for Karpenter, allowing flexibility in node placement.

**Dependencies:** None (foundational module)

---

### 3.2 IAM_EKS Module (`terraform/modules/IAM_EKS/`)

**Purpose:** Creates IAM roles for the EKS cluster and worker nodes

**What it creates:**
- `aws_iam_role.eks_cluster_role` - Role for EKS control plane
- `aws_iam_role.eks_node_role` - Role for worker nodes
- `aws_iam_instance_profile.eks_node_instance_profile` - Instance profile for nodes (required by Karpenter)

**Key Design Decision:** Separate roles for cluster vs nodes follows AWS best practices and least-privilege.

**Dependencies:** None (but requires assume role policy document)

---

### 3.3 EKS Module (`terraform/modules/EKS/`)

**Purpose:** Creates and configures the Kubernetes cluster

**What it creates:**
- EKS cluster (v1.31) with API-only authentication
- EKS access entry + policy for admin access
- EKS node group (baseline: 1-2 t3.medium nodes)
- **Critical:** Tags cluster security group with `karpenter.sh/discovery`
- Attaches IAM policies to cluster and node roles

**Key Design Decision:** 
- Uses `authentication_mode = "API"` (modern EKS access control)
- Creates minimal baseline node group (Karpenter handles scaling)
- Tags security group so Karpenter can discover it

**Dependencies:** 
- VPC module (subnet IDs)
- IAM_EKS module (role ARNs)

---

### 3.4 IAM Module (`terraform/modules/IAM/`)

**Purpose:** Creates IAM role for Karpenter controller with IRSA (IAM Roles for Service Accounts)

**What it creates:**
- IAM role with OIDC trust policy (allows Karpenter service account to assume role)
- Inline IAM policy (from JSON file) granting EC2, IAM, Launch Template permissions
- Policy attachment

**Key Design Decision:** Uses IRSA for secure, zero-secret authentication between Karpenter pods and AWS APIs.

**Dependencies:**
- EKS module (OIDC provider URL/ARN)
- External script (OIDC thumbprint calculation)

---

### 3.5 OIDC Provider (`terraform/environments/dev/main.tf`)

**Purpose:** Enables IRSA by creating an OIDC identity provider

**What it creates:**
- `aws_iam_openid_connect_provider` linked to EKS cluster's OIDC issuer
- Uses Python script to fetch SSL certificate thumbprint

**Key Design Decision:** External script with error handling ensures robust thumbprint calculation.

**Dependencies:**
- EKS module (OIDC issuer URL)
- Python script (`get-oidc-thumbprint.py`)

---

### 3.6 Karpenter Provisioner (`Karpenter/main.yml`)

**Purpose:** Defines how Karpenter should provision nodes

**Configuration:**
- **Instance Types:** t3.medium, t3.large, t3.xlarge
- **Capacity Type:** On-demand (avoids Spot service-linked role requirement)
- **Discovery:** Uses tags `karpenter.sh/discovery: aura-eks-dev`
- **TTL:** 60 seconds after nodes become empty (aggressive cost optimization)

**Key Design Decision:** On-demand instances avoid Spot permission complexity while maintaining cost control via aggressive scale-down.

---

### 3.7 CI/CD Workflow (`.github/workflows/cd-cd.yml`)

**Purpose:** Fully automated deployment and teardown pipeline

**Workflow Steps:**

```
1. Checkout Code
2. Configure AWS Credentials (OIDC-based, no secrets!)
3. Terraform Init/Plan/Apply
   â””â”€ Creates: VPC â†’ EKS â†’ IAM â†’ OIDC Provider
4. Get Terraform Outputs (cluster name, endpoint, role ARNs)
5. Configure kubectl (connect to EKS cluster)
6. Deploy Karpenter (Helm chart)
7. Wait for Karpenter Ready (with health checks)
8. Apply Karpenter Provisioner (with webhook workaround)
9. Deploy Application Job
10. Wait for Job Completion (if deploy-and-destroy)
11. Collect Comprehensive Metrics
12. Terraform Destroy (if not 'deploy' only)
13. Generate & Upload Summary Report
```

**Key Features:**
- **OIDC Authentication:** No AWS credentials stored in GitHub
- **Comprehensive Reporting:** Collects nodes, pods, events, logs
- **Error Handling:** Detailed logging and debugging output
- **Flexible Actions:** deploy, deploy-and-destroy, destroy

---

## 4. Data Flow & Communication

### 4.1 Infrastructure Provisioning Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Terraform Apply (terraform/environments/dev/main.tf)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VPC Module   â”‚    â”‚ IAM_EKS     â”‚    â”‚ (standalone) â”‚
â”‚ Creates:     â”‚    â”‚ Creates:    â”‚    â”‚              â”‚
â”‚ - VPC        â”‚    â”‚ - Cluster   â”‚    â”‚              â”‚
â”‚ - Subnets    â”‚    â”‚   Role      â”‚    â”‚              â”‚
â”‚ - NAT GW     â”‚    â”‚ - Node Role â”‚    â”‚              â”‚
â”‚ - Routes     â”‚    â”‚ - Instance  â”‚    â”‚              â”‚
â”‚              â”‚    â”‚   Profile   â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
       â”‚                   â”‚ (subnet_ids, role_arns)
       â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   EKS Module    â”‚
          â”‚ Creates:        â”‚
          â”‚ - EKS Cluster   â”‚
          â”‚ - Node Group    â”‚
          â”‚ - Access Entry  â”‚
          â”‚ - Security Groupâ”‚
          â”‚   (tagged)      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ (oidc_issuer_url)
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ OIDC Provider   â”‚
          â”‚ (uses Python    â”‚
          â”‚  script for     â”‚
          â”‚  thumbprint)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ (oidc_provider_arn, url)
                   â”‚
                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   IAM Module    â”‚
          â”‚ Creates:        â”‚
          â”‚ - Karpenter     â”‚
          â”‚   Controller    â”‚
          â”‚   Role (IRSA)   â”‚
          â”‚ - Inline Policy â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Karpenter Autoscaling Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User/CI/CD Creates Kubernetes Pod/Job                    â”‚
â”‚    kubectl apply -f app-job.yml                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Kubernetes Scheduler: Pod Status = Pending               â”‚
â”‚    (No nodes have capacity)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Karpenter Controller Detects Pending Pod                 â”‚
â”‚    - Watches Kubernetes API                                 â”‚
â”‚    - Evaluates Provisioner requirements                     â”‚
â”‚    - Calculates: "Need 1 node with 3 CPU, 3Gi RAM"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Karpenter Discovers Resources                            â”‚
â”‚    - Queries AWS EC2: Subnets with tag                      â”‚
â”‚      karpenter.sh/discovery: aura-eks-dev                  â”‚
â”‚    - Queries AWS EC2: Security Groups with same tag         â”‚
â”‚    - Selects instance type: t3.xlarge (fits requirements)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Karpenter Creates Launch Template                        â”‚
â”‚    - Uses node instance profile (from Terraform)           â”‚
â”‚    - Configures user-data for EKS node registration        â”‚
â”‚    - Tags instance with Karpenter labels                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Karpenter Launches EC2 Instance                         â”‚
â”‚    - Calls EC2 RunInstances API                             â”‚
â”‚    - Instance boots in private subnet                      â”‚
â”‚    - Node registers with EKS cluster                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Kubernetes Scheduler Places Pod on New Node              â”‚
â”‚    - Node becomes Ready                                     â”‚
â”‚    - Pod status: Pending â†’ Running                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. After Job Completes + 60s Empty                          â”‚
â”‚    Karpenter Terminates Empty Node                          â”‚
â”‚    (ttlSecondsAfterEmpty: 60)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 CI/CD Execution Flow

```
GitHub Actions Workflow Trigger
    â”‚
    â”œâ”€â†’ Checkout Code
    â”œâ”€â†’ AWS OIDC Auth (assume role via GitHub identity)
    â”œâ”€â†’ Terraform Init
    â”œâ”€â†’ Terraform Plan
    â”œâ”€â†’ Terraform Apply
    â”‚   â””â”€â†’ Creates all infrastructure (VPC â†’ EKS â†’ IAM â†’ OIDC)
    â”‚
    â”œâ”€â†’ Get Terraform Outputs (JSON)
    â”œâ”€â†’ Configure kubectl (aws eks update-kubeconfig)
    â”‚
    â”œâ”€â†’ Deploy Karpenter (Helm)
    â”‚   â””â”€â†’ Sets: clusterName, clusterEndpoint, role ARN, instance profile
    â”‚
    â”œâ”€â†’ Wait for Karpenter Ready (health checks)
    â”œâ”€â†’ Apply Provisioner (with webhook workaround)
    â”‚
    â”œâ”€â†’ Deploy App Job (kubectl apply)
    â”‚   â””â”€â†’ Karpenter provisions node automatically
    â”‚
    â”œâ”€â†’ Wait for Job Completion (if deploy-and-destroy)
    â”‚
    â”œâ”€â†’ Collect Metrics
    â”‚   â”œâ”€â†’ Nodes (JSON)
    â”‚   â”œâ”€â†’ Pods (JSON)
    â”‚   â”œâ”€â†’ Events (text)
    â”‚   â”œâ”€â†’ Karpenter logs
    â”‚   â””â”€â†’ Job logs
    â”‚
    â”œâ”€â†’ Generate Summary Report (Markdown)
    â”œâ”€â†’ Upload Artifacts
    â”‚
    â””â”€â†’ Terraform Destroy (if not 'deploy' only)
        â””â”€â†’ Tears down all resources
```

---

## 5. Tech Stack & Dependencies

### Infrastructure Layer

| Technology | Purpose | Why It's Used |
|------------|---------|---------------|
| **Terraform** | Infrastructure provisioning | Declarative IaC, state management, module reusability |
| **AWS EKS** | Kubernetes orchestration | Managed Kubernetes, AWS integration, scalability |
| **AWS VPC** | Network isolation | Security, subnet management, NAT gateway for private nodes |
| **AWS IAM** | Access control | Fine-grained permissions, IRSA for pod-to-AWS communication |
| **AWS EC2** | Compute instances | On-demand node provisioning via Karpenter |

### Kubernetes Layer

| Technology | Purpose | Why It's Used |
|------------|---------|---------------|
| **Karpenter** | Node autoscaler | Fast provisioning (<2min), cost-optimized, AWS-native |
| **Kubernetes Jobs** | Workload execution | Batch processing, one-time tasks, completion tracking |
| **Helm** | Package management | Karpenter installation, version pinning, configuration |

### CI/CD Layer

| Technology | Purpose | Why It's Used |
|------------|---------|---------------|
| **GitHub Actions** | Pipeline orchestration | Native GitHub integration, OIDC support, artifact storage |
| **AWS OIDC** | Authentication | No secrets, secure role assumption, audit trail |
| **kubectl** | Cluster management | Kubernetes API access, resource deployment |

### Supporting Tools

| Technology | Purpose |
|------------|---------|
| **Python 3** | OIDC thumbprint calculation |
| **jq** | JSON parsing in CI/CD |
| **bash** | Scripting and automation |

---

## 6. Execution Flow Example

### Scenario: Running an AI/ML Job via CI/CD

**Step-by-Step Walkthrough:**

```
1. Developer triggers GitHub Actions workflow
   Input: action="deploy-and-destroy", job_name="example-job"
   
2. GitHub Actions authenticates to AWS
   - Uses OIDC token (no AWS credentials stored)
   - Assumes role: arn:aws:iam::125156866057:role/github-OICD
   
3. Terraform provisions infrastructure (~5-8 minutes)
   â”œâ”€ VPC created (10.0.0.0/16)
   â”œâ”€ Subnets created (public: 10.0.1.0/24, 10.0.2.0/24)
   â”‚                    (private: 10.0.3.0/24, 10.0.4.0/24)
   â”œâ”€ NAT Gateway created (for private subnet internet access)
   â”œâ”€ IAM roles created (cluster, node, Karpenter)
   â”œâ”€ EKS cluster created (aura-eks-dev)
   â”œâ”€ Baseline node group created (1x t3.medium)
   â”œâ”€ OIDC provider created (for IRSA)
   â””â”€ Security group tagged (karpenter.sh/discovery: aura-eks-dev)
   
4. Terraform outputs captured
   - cluster_name: "aura-eks-dev"
   - cluster_endpoint: "https://..."
   - karpenter_controller_role_arn: "arn:aws:iam::..."
   - node_instance_profile_name: "aura-eks-node-role-instance-profile"
   
5. kubectl configured
   aws eks update-kubeconfig --name aura-eks-dev
   
6. Karpenter deployed via Helm (~2 minutes)
   - Helm installs Karpenter chart (v0.16.3)
   - Controller pod starts
   - Webhook pod starts
   - Service account annotated with IAM role ARN
   
7. Karpenter Provisioner applied
   - CRD defines: instance types, capacity type, discovery tags
   - Webhooks temporarily disabled (v0.16.3 TLS workaround)
   
8. Application Job deployed
   kubectl apply -f Karpenter/app-job.yml
   - Job requests: 1 CPU, 1Gi memory
   - Pod status: Pending (baseline node may have capacity)
   
9. Karpenter evaluates provisioning needs
   - If baseline node has capacity: Pod schedules immediately
   - If not: Karpenter provisions new node
     â”œâ”€ Discovers subnets (tag: karpenter.sh/discovery)
     â”œâ”€ Discovers security group (tag: karpenter.sh/discovery)
     â”œâ”€ Selects instance type (t3.medium fits 1 CPU, 1Gi)
     â”œâ”€ Creates launch template
     â”œâ”€ Launches EC2 instance (~1-2 minutes)
     â””â”€ Node registers with EKS cluster
   
10. Job executes
    - Pod runs on provisioned node
    - Job completes (calculates pi to 2000 digits)
    - Logs collected
    
11. Scale-down (if node empty for 60s)
    - Karpenter detects empty node
    - Terminates EC2 instance
    - Node removed from cluster
    
12. Summary report generated
    - Node counts, pod status, resource utilization
    - Karpenter events, job logs
    - Errors/warnings
    
13. Terraform destroy (if deploy-and-destroy)
    - All resources deleted
    - Zero cost after completion
```

---

## 7. Strengths & Tradeoffs

### âœ… Strengths

1. **Modularity & Reusability**
   - Terraform modules are self-contained and reusable
   - Easy to add new environments (prod, staging) by copying `dev/`
   - Clear separation: VPC, EKS, IAM responsibilities

2. **Security Best Practices**
   - OIDC-based authentication (no secrets in GitHub)
   - IRSA for pod-to-AWS communication (no IAM keys)
   - Least-privilege IAM policies
   - Private subnets for worker nodes

3. **Cost Optimization**
   - Ephemeral infrastructure (destroyed after use)
   - Aggressive scale-down (60s empty TTL)
   - Minimal baseline nodes (Karpenter handles scaling)
   - On-demand instances (predictable costs)

4. **Observability**
   - Comprehensive CI/CD summary reports
   - Detailed logging and event collection
   - Artifact storage for debugging

5. **Automation**
   - Fully automated lifecycle (deploy â†’ run â†’ destroy)
   - No manual intervention required
   - Reproducible deployments

6. **Error Handling**
   - Python script with try/except blocks
   - Health checks for Karpenter readiness
   - Detailed error messages in logs

### âš ï¸ Tradeoffs & Limitations

1. **Initial Setup Complexity**
   - Multiple Terraform modules to understand
   - OIDC provider setup requires thumbprint calculation
   - Karpenter webhook workaround needed (v0.16.3 limitation)

2. **State Management**
   - Ephemeral by design (state destroyed after run)
   - Not suitable for persistent workloads
   - Requires re-provisioning for each run

3. **Cold Start Time**
   - Infrastructure provisioning: ~5-8 minutes
   - Node provisioning: ~1-2 minutes
   - Total time to first pod: ~7-10 minutes

4. **Instance Type Limitations**
   - Currently limited to t3 family (medium, large, xlarge)
   - No GPU instance types configured (despite project name)
   - On-demand only (no Spot for cost savings)

5. **Single Region/AZ**
   - Hardcoded to us-east-1
   - Limited to 2 availability zones
   - No multi-region support

6. **Karpenter Version**
   - Using v0.16.3 (older version)
   - Webhook TLS issues require workaround
   - Missing newer Karpenter features

### ğŸ”§ Areas for Improvement

1. **Add GPU Support**
   - Add GPU instance types (g4dn, g5) to Provisioner
   - Configure NVIDIA device plugin
   - Update documentation

2. **Spot Instance Support**
   - Add Spot capacity type option
   - Create EC2 Spot service-linked role
   - Configure interruption handling

3. **Multi-Environment Support**
   - Parameterize region/AZ
   - Environment-specific variable files
   - Separate Terraform workspaces

4. **Upgrade Karpenter**
   - Migrate to newer version (v1.x)
   - Remove webhook workaround
   - Leverage new features

5. **Enhanced Monitoring**
   - CloudWatch integration
   - Prometheus metrics export
   - Cost tracking per run

---

## 8. Final Summary

**In 2-3 sentences:**

This is a **fully automated, ephemeral Kubernetes infrastructure system** that uses Terraform to provision AWS EKS clusters with Karpenter autoscaling, runs AI/ML workloads on-demand, and tears everything down when doneâ€”achieving **zero idle cost**. The architecture is **modular and secure** (OIDC authentication, IRSA for pods, least-privilege IAM), with a **comprehensive CI/CD pipeline** that handles the entire lifecycle from infrastructure creation to job execution to resource cleanup, complete with detailed reporting.

**Key Value Proposition:**
> Spin up a production-grade Kubernetes cluster, run your AI workload, get results, and pay only for what you useâ€”all with a single GitHub Actions workflow trigger.

---

## Appendix: Key Files Reference

| File | Purpose |
|------|---------|
| `terraform/environments/dev/main.tf` | Root module - wires all components together |
| `terraform/modules/VPC/main.tf` | Network infrastructure (VPC, subnets, NAT) |
| `terraform/modules/EKS/main.tf` | Kubernetes cluster + node group + access control |
| `terraform/modules/IAM/main.tf` | Karpenter controller IAM role (IRSA) |
| `terraform/modules/IAM_EKS/main.tf` | EKS cluster/node IAM roles |
| `Karpenter/main.yml` | Karpenter Provisioner CRD (defines scaling behavior) |
| `Karpenter/app-job.yml` | Example Kubernetes Job manifest |
| `.github/workflows/cd-cd.yml` | CI/CD pipeline (deploy â†’ run â†’ destroy) |
| `terraform/environments/dev/get-oidc-thumbprint.py` | Helper script for OIDC provider setup |

---

*Document generated: 2026-01-21*  
*Architecture version: 1.0*
