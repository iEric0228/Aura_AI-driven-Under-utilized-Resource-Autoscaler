name: Ephemeral EKS Job

permissions:
  id-token: write
  contents: read

on: 
  workflow_dispatch:
    inputs:
      action:
        description: "Deployment action"
        required: true
        default: "deploy-and-destroy"
        type: choice
        options:
          - deploy-and-destroy
          - deploy
          - destroy 
      job_name:
        description: "Kubernetes Job name to wait for"
        required: false
        default: "example-job"
      job_namespace:
        description: "Kubernetes namespace for the job"
        required: false
        default: "default"

jobs:
  main:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: us-east-1
    steps: 
      - name: Checkout 
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::125156866057:role/github-OICD
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        if: ${{ github.event.inputs.action != 'destroy' }}
        working-directory: ./terraform/environments/dev
        run: terraform init

      - name: Terraform Plan 
        if: ${{ github.event.inputs.action != 'destroy' }}
        working-directory: ./terraform/environments/dev
        run: terraform plan -out=tfplan

      - name: Terraform Apply 
        if: ${{ github.event.inputs.action != 'destroy' }}
        working-directory: ./terraform/environments/dev
        run: terraform apply -auto-approve
        continue-on-error: false

      - name: Get Terraform Outputs
        if: ${{ github.event.inputs.action != 'destroy' }}
        id: tfoutputs
        working-directory: ./terraform/environments/dev
        run: terraform output -json > tf-outputs.json
        continue-on-error: false

      - name: Set up kubectl
        if: ${{ github.event.inputs.action != 'destroy' }}
        run: |
          aws eks update-kubeconfig --region $AWS_REGION --name $(jq -r .cluster_name.value tf-outputs.json)
        continue-on-error: false
      
      - name: Deploy Karpenter
        if: ${{ github.event.inputs.action != 'destroy' }}
        run: |
          helm repo add karpenter https://charts.karpenter.sh
          helm repo update
          helm install karpenter karpenter/karpenter \
            --namespace karpenter --create-namespace \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$(jq -r .karpenter_controller_role_arn.value tf-outputs.json) \
            --set controller.clusterName=$(jq -r .cluster_name.value tf-outputs.json) \
            --set controller.clusterEndpoint=$(jq -r .cluster_endpoint.value tf-outputs.json) \
            --set controller.aws.defaultInstanceProfile=$(jq -r .node_instance_profile_name.value tf-outputs.json)
          kubectl apply -f ./Karpenter/main.yaml
        continue-on-error: false

      - name: Apply Karpenter Provisioner
        if: ${{ github.event.inputs.action != 'destroy' }}
        run: kubectl apply -f ./Karpenter/main.yml
        continue-on-error: false

      - name: Deploy App/Job
        if: ${{ github.event.inputs.action != 'destroy' }}
        run: kubectl apply -f ./app-job.yaml
        continue-on-error: false

      - name: Wait for Job Completion
        if: ${{ github.event.inputs.action == 'deploy-and-destroy' }}
        run: |
          kubectl wait --for=condition=complete --timeout=600s job/${{ github.event.inputs.job_name }} -n ${{ github.event.inputs.job_namespace }}
          kubectl logs job/${{ github.event.inputs.job_name }} -n ${{ github.event.inputs.job_namespace }} > job-logs.txt || echo "No logs found" > job-logs.txt
        continue-on-error: false

      - name: Terraform Destroy 
        if: ${{ github.event.inputs.action != 'deploy' }}
        working-directory: ./terraform/environments/dev
        run: terraform destroy -auto-approve
        continue-on-error: false

      - name: Generate Summary
        if: always()
        run: |
          echo "### CI/CD Summary" > summary.md
          echo "- **Action:** ${{ github.event.inputs.action }}" >> summary.md
          echo "- **Job Name:** ${{ github.event.inputs.job_name }}" >> summary.md
          echo "- **Job Namespace:** ${{ github.event.inputs.job_namespace }}" >> summary.md
          echo "- **Workflow Status:** ${{ job.status }}" >> summary.md
          echo "- **Cluster Name:** $(jq -r .cluster_name.value tf-outputs.json)" >> summary.md
          echo "- **Cluster Endpoint:** $(jq -r .cluster_endpoint.value tf-outputs.json)" >> summary.md
          echo "- **Karpenter Controller Role ARN:** $(jq -r .karpenter_controller_role_arn.value tf-outputs.json)" >> summary.md
          echo "- **Node Instance Profile Name:** $(jq -r .node_instance_profile_name.value tf-outputs.json)" >> summary.md
          echo "--- Job Logs ---" >> summary.md
          cat job-logs.txt >> summary.md
        shell: bash

      - name: Upload Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: summary
          path: summary.md